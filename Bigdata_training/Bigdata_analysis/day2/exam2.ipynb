{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 정적 웹 크롤링과 스크래핑을 이용한 공공데이터와 SNS 데이터 수집"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import urllib.request as req\n",
    "import io\n",
    "\n",
    "url = \"http://www.kma.go.kr/weather/forecast/mid-term-rss3.jsp?stnId=108\"\n",
    "savename = \"C:/Temp/forecast.xml\"\n",
    "req.urlretrieve(url, savename)\n",
    "\n",
    "xml = open(savename, \"r\", encoding=\"utf-8\").read()\n",
    "soup = BeautifulSoup(xml, 'xml') # xml\n",
    "\n",
    "info = {}\n",
    "for location in soup.find_all(\"location\"):\n",
    "    loc = location.find('city').string\n",
    "    min_w = location.find_all('tmn')\n",
    "    max_w = location.find_all('tmx')\n",
    "    weather = [a.string+\"~\"+b.string for a, b in zip(min_w, max_w)]\n",
    "\n",
    "    if not (loc in info):\n",
    "        info[loc] = []\n",
    "    for data in weather:\n",
    "        info[loc].append(data)\n",
    "print(info)\n",
    "\n",
    "with open('C:/Temp/forecast.txt', \"wt\", encoding=\"utf-8\") as f:\n",
    "    for loc in sorted(info.keys()):\n",
    "        f.write(str(loc)+'\\n')\n",
    "        for name in info[loc]:\n",
    "            f.write('\\t'+str(name)+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib.request\n",
    "\n",
    "now = datetime.datetime.now()\n",
    "nowDate = now.strftime('%Y년 %m월 %d일 %H시 %M분 입니다.')\n",
    "print(nowDate)\n",
    "# 오늘의 날씨\n",
    "print('[ 오늘의 날씨 요약 ]')\n",
    "webpage = urllib.request.urlopen('https://search.naver.com/search.naver?sm=top_hty&fbm=0&ie=utf8&query='+urllib.parse.quote_plus('서울날씨'))\n",
    "soup = BeautifulSoup(webpage, 'html.parser')\n",
    "temp = soup.select('div.temperature_text > strong')\n",
    "cast = soup.select('span.weather.before_slash')\n",
    "print(f'--> 서울 날씨 : {temp[0].contents[1]}℃ {cast[0].text}')\n",
    "\n",
    "webpage = urllib.request.urlopen('https://search.naver.com/search.naver?sm=top_hty&fbm=0&ie=utf8&query='+urllib.parse.quote_plus('광주날씨'))\n",
    "soup = BeautifulSoup(webpage, 'html.parser')\n",
    "temp = soup.select('div.temperature_text > strong')\n",
    "cast = soup.select('span.weather.before_slash')\n",
    "print(f'--> 광주 날씨 : {temp[0].contents[1]}℃ {cast[0].text}')\n",
    "\n",
    "webpage = urllib.request.urlopen('https://search.naver.com/search.naver?sm=top_hty&fbm=0&ie=utf8&query='+urllib.parse.quote_plus('제주도날씨'))\n",
    "soup = BeautifulSoup(webpage, 'html.parser')\n",
    "temp = soup.select('div.temperature_text > strong')\n",
    "cast = soup.select('span.weather.before_slash')\n",
    "print(f'--> 제주도 날씨 : {temp[0].contents[1]}℃ {cast[0].text}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import urllib.request as req\n",
    "\n",
    "busNum = '360'\n",
    "key = '%2BjzsSyNtwmcqxUsGnflvs3rW2oceFvhHR8AFkM3ao%2Fw50hwHXgGyPVutXw04uAXvrkoWgkoScvvhlH7jgD4%2FRQ%3D%3D'\n",
    "url1 = 'http://ws.bus.go.kr/api/rest/busRouteInfo/getBusRouteList?serviceKey='+key+'&strSrch='+busNum\n",
    "res = req.urlopen(url1)\n",
    "\n",
    "soup = BeautifulSoup(res.read().decode('utf-8'), \"xml\")\n",
    "\n",
    "busRouteId = None\n",
    "for itemList in soup.find_all('itemList') :\n",
    "    busRouteId = itemList.find('busRouteId').string\n",
    "    busRouteNm = itemList.find('busRouteNm').string\n",
    "    if busRouteNm == busNum :\n",
    "        break\n",
    "\n",
    "url2 = 'http://ws.bus.go.kr/api/rest/busRouteInfo/getStaionByRoute?ServiceKey='+key+'&busRouteId='+busRouteId\n",
    "res = req.urlopen(url2)\n",
    "soup = BeautifulSoup(res.read(), \"xml\")\n",
    "\n",
    "busPos = []\n",
    "for itemList in soup.find_all('itemList') :\n",
    "    gpsY = itemList.find('gpsY').string\n",
    "    gpsX = itemList.find('gpsX').string\n",
    "\n",
    "    busPos.append((gpsY, gpsX))\n",
    "\n",
    "print('[ ' + busNum + '번 버스의 운행 위치 ]')\n",
    "if len(busPos) != 0 :\n",
    "    print(busNum + '번 버스 ' + str(len(busPos)) + '대 운행중...')\n",
    "    for lat,lng in busPos :\n",
    "        print(lat+','+lng)\n",
    "else :\n",
    "    print('현재 운행중인 ' + busNum + '번 버스가 없어요...')\n",
    "\n",
    "print(url1)\n",
    "print(url2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import urllib.request as req\n",
    "\n",
    "key = '796143536a756e69313134667752417a'\n",
    "contentType = 'xml'\n",
    "startIndex = '1'\n",
    "endIndex = '100'\n",
    "url = 'http://openapi.seoul.go.kr:8088/'+key+'/'+contentType+'/LampScpgmtb/'+startIndex+'/'+endIndex+'/'\n",
    "res = req.urlopen(url)\n",
    "xml = res.read()\n",
    "soup = BeautifulSoup(xml, \"xml\")\n",
    "\n",
    "pjList = []\n",
    "for itemList in soup.find_all('row') :\n",
    "    up_nm = itemList.find('UP_NM').string\n",
    "    up_nm = '없음' if up_nm is None else up_nm\n",
    "    pgm_nm = itemList.find('PGM_NM').string\n",
    "    pgm_nm = '없음' if pgm_nm is None else pgm_nm\n",
    "    target_nm = itemList.find('TARGET_NM').string\n",
    "    target_nm = '없음' if target_nm is None else target_nm\n",
    "    u_price = itemList.find('U_PRICE').string\n",
    "    u_price = '없음' if u_price is None else u_price\n",
    "    pjList.append((up_nm, pgm_nm, target_nm, u_price))\n",
    "\n",
    "print('[ 서울 청소년 수련관 강좌 리스트 ]')\n",
    "for up_nm,pgm_nm,target_nm,u_price in pjList :\n",
    "    print(up_nm+','+pgm_nm+','+target_nm+','+str(u_price))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import urllib.request as req\n",
    "import io\n",
    "\n",
    "key = '796143536a756e69313134667752417a'\n",
    "contentType = 'xml'\n",
    "startIndex = '1'\n",
    "endIndex = '100'\n",
    "date = '20220815'\n",
    "\n",
    "url = 'http://openapi.seoul.go.kr:8088/'+key+'/'+contentType+'/CardSubwayStatsNew/'+startIndex+'/'+endIndex+'/'+date+'/'\n",
    "print(url)\n",
    "res = req.urlopen(url)\n",
    "xml = res.read()\n",
    "\n",
    "soup = BeautifulSoup(xml, \"xml\")\n",
    "\n",
    "subwayList = []\n",
    "for itemList in soup.find_all('row') :\n",
    "    line_num = itemList.find('LINE_NUM').string\n",
    "    sub_sta_nm = itemList.find('SUB_STA_NM').string\n",
    "    ride_pasgr_num = itemList.find('RIDE_PASGR_NUM').string\n",
    "    alight_pasgr_num = itemList.find('ALIGHT_PASGR_NUM').string\n",
    "    subwayList.append((line_num, sub_sta_nm, ride_pasgr_num, alight_pasgr_num))\n",
    "\n",
    "print('[ 서울시 지하철호선별 역별 승하차 인원 정보 ]')\n",
    "for line_num, sub_sta_nm, ride_pasgr_num, alight_pasgr_num in subwayList :\n",
    "    print(line_num+','+sub_sta_nm+','+ride_pasgr_num+','+alight_pasgr_num)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import json\n",
    "client_key = 'izGsqP2exeThwwEUVU3x'\n",
    "client_secret = 'WrwbQ1l6ZI'\n",
    "query = '스마트폰'\n",
    "encText = urllib.parse.quote_plus(query)\n",
    "num = 100\n",
    "naver_url = 'https://openapi.naver.com/v1/search/blog.json?query=' + encText + '&display=' + str(num)\n",
    "\n",
    "request = urllib.request.Request(naver_url)\n",
    "request.add_header(\"X-Naver-Client-Id\",client_key)\n",
    "request.add_header(\"X-Naver-Client-Secret\",client_secret)\n",
    "response = urllib.request.urlopen(request)\n",
    "rescode = response.getcode()\n",
    "if(rescode == 200):\n",
    "    response_body = response.read()\n",
    "    dataList = json.loads(response_body)\n",
    "\n",
    "    print('[' + query + '에 대한 네이버 블로그 글 ]')\n",
    "    for count, data in enumerate(dataList['items'], 1) :\n",
    "        print (str(count) + ' : ' + data['title'])\n",
    "        print ('[' + data['description'] + ']')\n",
    "else:\n",
    "    print('오류 코드 : ' + rescode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataList # JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataList[\"items\"][0][\"description\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title = []\n",
    "desc = []\n",
    "for data in dataList[\"items\"]:\n",
    "    title.append(data[\"title\"])\n",
    "    desc.append(data[\"description\"])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "len(desc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "for i in range(len(title)) :\n",
    "   title[i] = re.sub(\"</?b>\", \"\", title[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(desc)) :\n",
    "   desc[i] = re.sub(\"</?b>\", \"\", desc[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "dictdata = {\"title\":title,\"description\":desc}\n",
    "df = pd.DataFrame(dictdata)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"output/smartphone.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import json\n",
    "\n",
    "client_key = 'izGsqP2exeThwwEUVU3x'\n",
    "client_secret = 'WrwbQ1l6ZI'\n",
    "query = '오징어게임'\n",
    "encText = urllib.parse.quote_plus(query)\n",
    "\n",
    "num = 100\n",
    "naver_url = 'https://openapi.naver.com/v1/search/news.json?query=' + encText + '&display=' + str(num)\n",
    "\n",
    "request = urllib.request.Request(naver_url)\n",
    "request.add_header(\"X-Naver-Client-Id\",client_key)\n",
    "request.add_header(\"X-Naver-Client-Secret\",client_secret)\n",
    "response = urllib.request.urlopen(request)\n",
    "\n",
    "rescode = response.getcode()\n",
    "\n",
    "if(rescode == 200):\n",
    "    response_body = response.read()\n",
    "    dataList = json.loads(response_body)\n",
    "    count = 1\n",
    "    print('[' + query + '에 대한 네이버 뉴스 글 ]')\n",
    "    for data in dataList['items'] :\n",
    "        print (str(count) + ' : ' + data['title'])\n",
    "        print ('[' + data['description'] + ']')\n",
    "        count += 1\n",
    "else:\n",
    "    print('오류 코드 : ' + rescode)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eduvenv",
   "language": "python",
   "name": "eduvenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
