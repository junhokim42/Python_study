{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 정적 웹 크롤링과 스크래핑"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style='color:red'>urllib.request.urlopen()</span> 함수로 웹페이지 소스 크롤링하기 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "res = urllib.request.urlopen(\"http://www.naver.com/\")\n",
    "print(type(res))\n",
    "print(res.status)\n",
    "print(\"NAVER 웹페이지의 소스 내용----------------------------------------------------------------\")\n",
    "print(res.read().decode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "res = urllib.request.urlopen(\"http://unico2013.dothome.co.kr/crawling/tagstyle.html\")\n",
    "print(res)\n",
    "print(\"[ header 정보 ]----------\")\n",
    "res_header = res.getheaders()\n",
    "for s in res_header :\n",
    "    print(s)\n",
    "print(\"[ body 내용 ]-----------\")\n",
    "print(res.read()) # 바이트 문자열, 바이트열 - b'..............'\n",
    "#print(res.read().decode('utf-8'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "print(\"===========================================================\")\n",
    "url = 'https://www.python.org/'\n",
    "f = urllib.request.urlopen(url)\n",
    "print(type(f))\n",
    "print(type(f.info()))\n",
    "encoding = f.info().get_content_charset()\n",
    "encoding = 'utf-8' if encoding == None else encoding\n",
    "print(url, ' 페이지의 인코딩 정보 :', encoding)\n",
    "text = f.read(500).decode(encoding)\n",
    "print(text)\n",
    "print(\"===========================================================\")\n",
    "\n",
    "url = 'https://www.daum.net/'\n",
    "f = urllib.request.urlopen(url)\n",
    "encoding = f.info().get_content_charset() \n",
    "encoding = 'utf-8' if encoding == None else encoding\n",
    "print(url, ' 페이지의 인코딩 정보 :', encoding)\n",
    "text = f.read(500).decode(encoding)\n",
    "print(text)\n",
    "print(\"===========================================================\")\n",
    "\n",
    "url = 'http://www.yes24.com/Main/default.aspx'\n",
    "f = urllib.request.urlopen(url)\n",
    "encoding = f.info().get_content_charset()\n",
    "encoding = 'utf-8' if encoding == None else encoding\n",
    "print(url, ' 페이지의 인코딩 정보 :', encoding)\n",
    "\n",
    "text = f.read(500).decode(encoding)\n",
    "print(text)\n",
    "print(\"===========================================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### urllib.parse.urlencode() 함수로 Query 문자열(요청 파라미터) 인코딩하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib\n",
    "print(\"[ Query문자열 또는 요청 파라미터 인코딩 ]\")\n",
    "params1 = urllib.parse.urlencode({'number': 12524, 'type': 'issue', 'action': 'show'})\n",
    "print(params1)\n",
    "params2 = urllib.parse.urlencode({'addr': '서울시 강남구 역삼동'})\n",
    "print(params2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib\n",
    "print(urllib.parse.quote_plus('가나다 ABC123'))\n",
    "query_str1 = \"q=\"+urllib.parse.quote_plus('가나 다')\n",
    "query_str2 = urllib.parse.urlencode({\"q\" : \"가나 다\"})\n",
    "print(\"-------------------------------------------\")\n",
    "print(query_str1)\n",
    "print(query_str2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query 문자열 전달하면서 GET 방식으로 요청하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import urllib.parse\n",
    "params = urllib.parse.urlencode({'name': '유니코', 'age': 10})\n",
    "print(\"URL 인코딩 규칙이 적용된 문자열 : %s\" % params)\n",
    "url = \"http://unico2013.dothome.co.kr/crawling/get.php?%s\" % params\n",
    "print(\"URL 문자열 : %s\" % url)\n",
    "with urllib.request.urlopen(url) as f:\n",
    "    print(f.read().decode('utf-8'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 요청 파라미터 전달하면서 POST 방식으로 요청하기(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import urllib.parse\n",
    "data = urllib.parse.urlencode({'name': '유니코', 'age': 10})\n",
    "print(\"URL 인코딩 규칙이 적용된 문자열 : %s\" % data)\n",
    "postdata = data.encode('ascii')\n",
    "print(\"변환된 바이트 문자열 : %s\" % postdata)\n",
    "url = \"http://unico2013.dothome.co.kr/crawling/post.php\"\n",
    "with urllib.request.urlopen(url, postdata) as f:\n",
    "    print(f.read().decode('utf-8'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 요청 파라미터 전달하면서 POST 방식으로 요청하기(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import urllib.parse\n",
    "data = urllib.parse.urlencode({'name': '유니코', 'age': 10})\n",
    "postdata = data.encode('ascii')\n",
    "req = urllib.request.Request(url='http://unico2013.dothome.co.kr/crawling/post.php',\n",
    "            data=postdata)\n",
    "print(req)\n",
    "with urllib.request.urlopen(req) as f:\n",
    "    print(f.read().decode('utf-8'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style='color:red'>requests.request()</span> 함수로 웹페이지 소스 크롤링하기 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "r = requests.request('get', 'http://unico2013.dothome.co.kr/crawling/exam.html')\n",
    "r.encoding = 'utf-8'\n",
    "print(type(r))\n",
    "if r.text :\n",
    "    print(r.text)\n",
    "else :\n",
    "    print('응답된 콘텐츠가 없어요')\n",
    "print('----------------------------------------------------------')\n",
    "r = requests.request('head', 'http://unico2013.dothome.co.kr/crawling/exam.html')\n",
    "r.encoding = 'utf-8'\n",
    "print(type(r))\n",
    "if r.text :\n",
    "    print(r.text)\n",
    "else :\n",
    "    print('응답된 콘텐츠가 없어요')\n",
    "print('----------------------------------------------------------')\n",
    "r = requests.request('post', 'http://unico2013.dothome.co.kr/crawling/post.php', data= {'name':'백도', 'age' : 12})\n",
    "r.encoding = 'utf-8'\n",
    "print(type(r))\n",
    "if r.text :\n",
    "    print(r.text)\n",
    "else :\n",
    "    print('응답된 콘텐츠가 없어요')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "r = requests.get('http://unico2013.dothome.co.kr/crawling/exam.html')\n",
    "r.encoding = 'utf-8'\n",
    "print(type(r))\n",
    "print(r.headers)\n",
    "if r.text :\n",
    "    print(r.text)\n",
    "else :\n",
    "    print('응답된 콘텐츠가 없어요')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "dicdata = {'key1': 'value1', 'key2': 'value2'}\n",
    "urlstr = 'http://unico2013.dothome.co.kr/crawling/get.php'\n",
    "r = requests.get(urlstr, params=dicdata)\n",
    "print(r.url)\n",
    "print('------------------------------------')\n",
    "dicdata = {'key1': 'value1', 'key2': ['value2', 'value3']}\n",
    "r = requests.request('GET', urlstr, params=dicdata)\n",
    "print(r.url)\n",
    "print('------------------------------------')\n",
    "tupledata = [('key1', 'value1'), ('key1', 'value2')]\n",
    "r = requests.get(urlstr, params=tupledata)\n",
    "print(r.url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "urlstr = 'http://unico2013.dothome.co.kr/crawling/get.php'\n",
    "r = requests.get(urlstr)\n",
    "print(r.text)\n",
    "print('------------------------------------')\n",
    "r.encoding = 'utf-8'\n",
    "print(r.text)\n",
    "print('------------------------------------')\n",
    "print(r.content)\n",
    "print('------------------------------------')\n",
    "print(r.content.decode('utf-8'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 이미지 크롤링하여 파일로 저장하기 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "\n",
    "r = requests.get('http://unico2013.dothome.co.kr/image/flower.jpg')\n",
    "i = Image.open(BytesIO(r.content))\n",
    "print(type(i))\n",
    "i.save(\"c:/Temp/test.jpg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 크롤링한 웹페이지 내용을 BeautifulSoup으로 스크래핑하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "html_doc = \"\"\"\n",
    "<!DOCTYPE html>\n",
    "<html>\n",
    "    <head>\n",
    "        <meta charset=\"utf-8\">\n",
    "        <title>Test BeautifulSoup</title>\n",
    "    </head>\n",
    "    <body>\n",
    "        <h1>테스트</h1>\n",
    "    </body>\n",
    "</html> \"\"\"\n",
    "from bs4 import BeautifulSoup\n",
    "bs = BeautifulSoup(html_doc, 'html.parser')\n",
    "print(type(bs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "html_doc = \"\"\"\n",
    "<!DOCTYPE html>\n",
    "<html>\n",
    "     <head>\n",
    "          <meta charset='utf-8'>\n",
    "          <title>Test BeautifulSoup</title>\n",
    "     </head>\n",
    "     <body>\n",
    "          <p align=\"center\">P태그의 컨텐트</p>\n",
    "          <img src=\"http://unico2013.dothome.co.kr/image/flower.jpg\" width=\"300\">\n",
    "     </body>\n",
    "</html> \"\"\"\n",
    "from bs4 import BeautifulSoup\n",
    "bs = BeautifulSoup(html_doc, 'html.parser')\n",
    "print(bs.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "html_doc = \"\"\"\n",
    "<!DOCTYPE html>\n",
    "<html>\n",
    "     <head>\n",
    "          <meta charset='utf-8'>\n",
    "          <title>Test BeautifulSoup</title>\n",
    "     </head>\n",
    "     <body>\n",
    "          <p align=\"center\">P태그의 컨텐트</p>\n",
    "          <img src=\"http://unico2013.dothome.co.kr/image/flower.jpg\" width=\"300\">\n",
    "          <ul>\n",
    "               <li>테스트1<strong>강조</strong></li>\n",
    "               <li>테스트2</li>\n",
    "               <li>테스트3</li>\n",
    "          </ul>\n",
    "     </body>\n",
    "</html> \"\"\"\n",
    "from bs4 import BeautifulSoup\n",
    "bs = BeautifulSoup(html_doc, 'html.parser')\n",
    "print(type(bs.title), ':', bs.title)\n",
    "print(type(bs.title.name), ':', bs.title.name)\n",
    "print(type(bs.title.string), ':', bs.title.string)\n",
    "print('-------------------------')\n",
    "print(type(bs.p['align']), ':', bs.p['align'])\n",
    "print(type(bs.img['src']), ':', bs.img['src'])\n",
    "print(type(bs.img.attrs), ':', bs.img.attrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "html_doc = \"\"\"\n",
    "<!DOCTYPE html>\n",
    "<html>\n",
    "     <head>\n",
    "          <meta charset='utf-8'>\n",
    "          <title>Test BeautifulSoup</title>\n",
    "     </head>\n",
    "     <body>\n",
    "          <p align=\"center\">P태그의 컨텐트</p>\n",
    "          <img src=\"http://unico2013.dothome.co.kr/image/flower.jpg\" width=\"300\">\n",
    "          <ul>\n",
    "               <li>테스트1<strong>강조</strong></li>\n",
    "               <li>테스트2</li>\n",
    "               <li>테스트3</li>\n",
    "          </ul>\n",
    "     </body>\n",
    "</html> \"\"\"\n",
    "from bs4 import BeautifulSoup\n",
    "bs = BeautifulSoup(html_doc, 'html.parser')\n",
    "print(\"[[[ string 속성 ]]]\")\n",
    "print(type(bs.p.string), ':', bs.p.string)\n",
    "print(type(bs.ul.string), ':', bs.ul.string)\n",
    "print(type(bs.ul.li.string), ':', bs.ul.li.string)\n",
    "print(type(bs.ul.li.strong.string), ':', bs.ul.li.strong.string)\n",
    "print(\"[[[ text 속성 ]]]\")\n",
    "print(type(bs.p.text), ':', bs.p.text)\n",
    "print(type(bs.ul.text), ':', bs.ul.text)\n",
    "print(type(bs.ul.li.text), ':', bs.ul.li.text)\n",
    "print(type(bs.ul.li.strong.text), ':', bs.ul.li.strong.text)\n",
    "print(\"[[[ contents 속성 ]]]\")\n",
    "print(type(bs.p.contents), ':', bs.p.contents)\n",
    "print(type(bs.ul.contents), ':', bs.ul.contents)\n",
    "print(type(bs.ul.li.contents), ':', bs.ul.li.contents)\n",
    "print(type(bs.ul.li.strong.contents), ':', bs.ul.li.strong.contents)\n",
    "print(\"[[[ find_all()의 text=True 매개변수 ]]]\")\n",
    "print(bs.find_all(\"li\", text=True))  # 콘텐츠가 텍스트로만 구성된"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "html_doc=\"\"\"\n",
    "<!DOCTYPE html>\n",
    "<html>\n",
    "  <head>\n",
    "     <meta charset='utf-8'>\n",
    "     <title>Test BeautifulSoup</title>\n",
    "  </head>\n",
    "  <body>\n",
    "     <p align=\"center\"> text contents </p>\n",
    "     <p align=\"right\">  text contents 2 </p>\n",
    "     <p align=\"left\">   text contents 3 </p>\n",
    "     <img src=\"http://unico2013.dothome.co.kr/image/flower.jpg\" width=\"500\">\n",
    "     <div>\n",
    "       <p>text contents 4</p> \n",
    "     </div>\n",
    "  </body>\n",
    "</html> \"\"\"\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "bs = BeautifulSoup(html_doc, 'html.parser')\n",
    "print(type(bs.find('p')))\n",
    "print(type(bs.find_all('p')))\n",
    "print(\"---------------------------------\")\n",
    "print(bs.find('title'))\n",
    "print(bs.find('p'))\n",
    "print(bs.find('img'))\n",
    "print(\"---------------------------------\")\n",
    "ptags = bs.find_all('p')\n",
    "print(ptags)\n",
    "print(\"----------------\")\n",
    "for tag in ptags :\n",
    "    print(tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "html=\"\"\"\n",
    "<!DOCTYPE html>\n",
    "<html>\n",
    "  <head>\n",
    "     <meta charset='utf-8'>\n",
    "     <title>Test BeautifulSoup</title>\n",
    "  </head>\n",
    "  <body>\n",
    "     <p align=\"center\"> text contents </p>\n",
    "     <p align=\"right\" class=\"myp\">  text contents 2 </p>\n",
    "     <p align=\"left\" a=\"b\">   text contents 3 </p>\n",
    "     <img src=\"http://unico2013.dothome.co.kr/image/flower.jpg\" width=\"500\">\n",
    "  </body>\n",
    "</html> \"\"\"\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "bs = BeautifulSoup(html, 'html.parser')\n",
    "print(bs.find('p', align=\"center\"))\n",
    "print(bs.find('p', class_=\"myp\"))\n",
    "print(bs.find('p', align=\"left\"))\n",
    "print(\"-------------------------------------\")\n",
    "print(bs.find('p', attrs={\"align\":\"center\"}))\n",
    "print(bs.find('p', attrs={\"align\":\"right\", \"class\":\"myp\"}))\n",
    "print(bs.find('p', attrs={\"align\":\"left\"}))\n",
    "\n",
    "pdom = bs.find('p', align=\"left\")\n",
    "print(\"[\"+pdom.text+\"]\")\n",
    "print(\"[\"+pdom.get_text(strip=True)+\"]\")\n",
    "print(\"[\"+pdom.string.strip()+\"]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "req = requests.get('http://unico2013.dothome.co.kr/crawling/exercise_css.html')\n",
    "html = req.content\n",
    "print(type(html))\n",
    "html = html.decode('utf-8')\n",
    "print(html)\n",
    "print(\"==============================\")\n",
    "bs = BeautifulSoup(html, 'html.parser')\n",
    "title = bs.select('h1')\n",
    "title1 = bs.select('#f_subtitle')\n",
    "title2 = bs.select('.subtitle')\n",
    "title3 = bs.select('aside > h2')\n",
    "img = bs.select('[src]')\n",
    "print(title)\n",
    "print(type(title))\n",
    "print(type(title[0]))\n",
    "print(\"<h1>태그의 갯수 : %d \" %len(title))\n",
    "print(\"f_subtitle이라는 id 속성을 갖는 태그 갯수 : %d \" %len(title1))\n",
    "print(\"subtitle이라는 class 속성을 갖는 태그 갯수 : %d \" %len(title2))\n",
    "print(\"aside 태그의 <h2> 자식 태그 갯수 : %d \" %len(title3))\n",
    "print(\"src 속성을 갖는 태그 갯수 : %d \" %len(img))\n",
    "\n",
    "for content in title:\n",
    "    print(content.string)\n",
    "print(\"------------------------------\")\n",
    "for content in title1:\n",
    "    print(content.text)\n",
    "print(\"------------------------------\")\n",
    "for content in title2:\n",
    "    print(content.text)\n",
    "print(\"------------------------------\")\n",
    "for content in title3:\n",
    "    print(content.text)\n",
    "print(\"------------------------------\")\n",
    "for content in img:\n",
    "   print(content[\"src\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "req = requests.get('http://movie.naver.com/movie/point/af/list.nhn?page=1')\n",
    "html = req.text\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "titles = soup.select('.movie')\n",
    "points = soup.select('td.title > div > em')\n",
    "reviews = soup.select('td.title')\n",
    "movie_title = []\n",
    "movie_point = []\n",
    "movie_review = [] \n",
    "\n",
    "for dom in titles:\n",
    "    movie_title.append(dom.text)\n",
    "for dom in points:\n",
    "    movie_point.append(dom.text)\n",
    "for dom in reviews:\n",
    "    content = dom.contents[6] \n",
    "    content=re.sub(\"[\\n\\t]\", '', content)    \n",
    "    movie_review.append(content)\n",
    "commentLength = len(movie_title)   \n",
    "for i in range(commentLength):\n",
    "    print(\"영화 제목 : \" + movie_title[i])\n",
    "    print(\"평점 : \" + movie_point[i])\n",
    "    print(\"리뷰글 : \" + movie_review[i])\n",
    "    print(\"-----------------------------------------\")   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "for n in range(1,31):\n",
    "    req = requests.get('http://movie.naver.com/movie/point/af/list.nhn?page='+str(n))\n",
    "    html = req.text\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    titles = soup.select('.movie' )\n",
    "    points = soup.select('td.title > div > em')\n",
    "    reviews = soup.select('td.title')\n",
    "    movie_title = []\n",
    "    movie_point = []\n",
    "    movie_review = [] \n",
    "    for dom in titles:\n",
    "        movie_title.append(dom.text)\n",
    "    for dom in points:\n",
    "        movie_point.append(dom.text)\n",
    "    for dom in reviews:\n",
    "        content = dom.contents[6]\n",
    "        content=re.sub(\"[\\n\\t]\", '', content)\n",
    "        movie_review.append(content)\n",
    "\n",
    "    commentLength = len(movie_title)   \n",
    "    for i in range(commentLength):\n",
    "        print(movie_point[i] + \",\"+movie_title[i]+\",\"+movie_review[i])\n",
    "    print(\"-----------------------------------------------------\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(reviews[0].contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "title = []\n",
    "link = []\n",
    "urlstr = 'http://www.yes24.com/SearchCorner/Search?domain=BOOK&query=python'\n",
    "r = requests.get(urlstr)\n",
    "#r.encoding = \"utf-8\"\n",
    "bs = BeautifulSoup(r.text, 'html.parser')\n",
    "bookList = bs.select('a.gd_name')\n",
    "\n",
    "for bookDom in bookList:\n",
    "    title.append(bookDom.string)\n",
    "    link.append(bookDom[\"href\"])\n",
    "\n",
    "print(\"-- 도서 제목 --\")\n",
    "print(title)\n",
    "print(\"-- 도서 링크 URL --\")\n",
    "print(link)\n",
    "with open('output/booklink.csv', \"wt\", encoding=\"utf-8\") as f:\n",
    "    f.write('booktitle,booklink\\n')  \n",
    "    for i in range(len(title)):\n",
    "        f.write(title[i]+\",\"+link[i]+'\\n')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "titles = list()\n",
    "summaries = list()\n",
    "ratings = list()\n",
    "\n",
    "for page_num in range(1,21):\n",
    "    url = f\"https://comic.naver.com/genre/bestChallenge.nhn?&page={page_num}\" \n",
    "    req = requests.get(url)\n",
    "    \n",
    "    html = req.content.decode(\"utf-8\")\n",
    "    bs= BeautifulSoup(html, \"html.parser\")\n",
    "    \n",
    "    for info in bs.find_all(class_=\"challengeInfo\"):\n",
    "        titles.append(info.find(class_=\"challengeTitle\").a.text.strip())\n",
    "        summaries.append(info.find(class_=\"summary\").text.strip())\n",
    "        ratings.append(info.find(class_=\"rating_type\").strong.text.strip())\n",
    "    print(f\"{len(titles)}개 추출\")\n",
    "    \n",
    "naver_comic_dict = { 'title':titles, 'summary' : summaries, 'rating' : ratings }    \n",
    "\n",
    "df = pd.DataFrame(naver_comic_dict)\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### API 별 응답내용 한글처리 방법"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "urlstr = 'http://unico2013.dothome.co.kr/crawling/get.php'\n",
    "r = requests.get(urlstr)\n",
    "r.encoding = 'utf-8'\n",
    "html = r.text\n",
    "print(html)\n",
    "print(\"===================================\")\n",
    "bs = BeautifulSoup(html, 'html.parser')\n",
    "print(bs.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "urlstr = 'http://unico2013.dothome.co.kr/crawling/get.php'\n",
    "r = requests.get(urlstr)\n",
    "html = r.content\n",
    "html = r.content.decode('utf-8')\n",
    "print(html)\n",
    "print(\"===================================\")\n",
    "bs = BeautifulSoup(html, 'html.parser')\n",
    "print(bs.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "from bs4 import BeautifulSoup\n",
    "urlstr = 'http://unico2013.dothome.co.kr/crawling/get.php'\n",
    "r = urllib.request.urlopen(urlstr)\n",
    "html = r.read()\n",
    "html = html.decode('utf-8')\n",
    "print(html)\n",
    "print(\"===================================\")\n",
    "bs = BeautifulSoup(html, 'html.parser')\n",
    "print(bs.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eduvenv",
   "language": "python",
   "name": "eduvenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
